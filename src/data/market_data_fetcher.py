"""
å¸‚åœºæ•°æ®è·å–å™¨
Market Data Fetcher

ä½¿ç”¨å¤šç§æ•°æ®æºè·å–ETFå†å²æ•°æ®ï¼Œè§£å†³ä»£ç†è¿æ¥é—®é¢˜
"""

import akshare as ak
import pandas as pd
import numpy as np
import os
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Union
import logging
from pathlib import Path
import pickle
import urllib.request
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# å¯¼å…¥jqdatasdk
try:
    import jqdatasdk
    from jqdatasdk import auth, get_price
    JQDATA_AVAILABLE = True
except ImportError:
    JQDATA_AVAILABLE = False
    jqdatasdk = None

# å¯¼å…¥xtquant
try:
    from xtquant import xtdata
    XTQUANT_AVAILABLE = True
except ImportError:
    XTQUANT_AVAILABLE = False
    xtdata = None

# å¯¼å…¥ä¸ªäººé…ç½®ç³»ç»Ÿ
try:
    from src.config.personal_config import get_personal_config
    PERSONAL_CONFIG_AVAILABLE = True
except ImportError:
    PERSONAL_CONFIG_AVAILABLE = False
    logger.warning("ä¸ªäººé…ç½®ç³»ç»Ÿä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")

# å¯¼å…¥æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨
try:
    from src.data.mock_benchmark_generator import mock_generator
    MOCK_GENERATOR_AVAILABLE = True
except ImportError:
    MOCK_GENERATOR_AVAILABLE = False
    logger.warning("æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨ä¸å¯ç”¨")

logger = logging.getLogger(__name__)


class MarketDataFetcher:
    """å¸‚åœºæ•°æ®è·å–å™¨ç±»"""

    def __init__(self, cache_dir: str = "cache", personal_config=None):
        """
        åˆå§‹åŒ–å¸‚åœºæ•°æ®è·å–å™¨

        Args:
            cache_dir: ç¼“å­˜ç›®å½•
            personal_config: ä¸ªäººé…ç½®ç®¡ç†å™¨å®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨å±€é…ç½®
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.cache_ttl = 86400  # 24å°æ—¶ç¼“å­˜

        # åˆå§‹åŒ–ä¸ªäººé…ç½®
        if personal_config:
            self.personal_config = personal_config
        elif PERSONAL_CONFIG_AVAILABLE:
            self.personal_config = get_personal_config()
        else:
            self.personal_config = None

        # é…ç½®ç½‘ç»œè®¾ç½®
        self._setup_network()

        # åˆå§‹åŒ–jqdatasdkï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        self._init_jqdata()

        # åˆå§‹åŒ–Tushareï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        self._init_tushare()

        # åˆå§‹åŒ–XTQuantï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        self._init_xtquant()

    def _setup_network(self):
        """é…ç½®ç½‘ç»œè¿æ¥ï¼Œç¦ç”¨ä»£ç†å¹¶è®¾ç½®é‡è¯•æœºåˆ¶"""
        # ç¦ç”¨æ‰€æœ‰ä»£ç†è®¾ç½®
        os.environ.update({
            'HTTP_PROXY': '',
            'HTTPS_PROXY': '',
            'http_proxy': '',
            'https_proxy': '',
            'NO_PROXY': '*',
            'no_proxy': '*'
        })

        # é…ç½®è¯·æ±‚sessionï¼Œæ·»åŠ é‡è¯•æœºåˆ¶
        self.session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=0.5,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        self.session.trust_env = False  # ä¸ä¿¡ä»»ç³»ç»Ÿä»£ç†

        # ä¸ºurllibç¦ç”¨ä»£ç†
        proxy_handler = urllib.request.ProxyHandler({})
        opener = urllib.request.build_opener(proxy_handler)
        urllib.request.install_opener(opener)

    def _init_jqdata(self):
        """åˆå§‹åŒ–jqdatasdk"""
        self.jqdata_initialized = False

        # æ£€æŸ¥æ˜¯å¦é…ç½®äº†jqdatasdk
        if self.personal_config and self.personal_config.is_data_source_enabled('jqdata') and JQDATA_AVAILABLE:
            try:
                # è·å–jqdatasdkå‡­è¯
                credentials = self.personal_config.get_api_credentials('jqdata')
                username = credentials.get('username')
                password = credentials.get('password')

                if username and password:
                    # ä½¿ç”¨jqdatasdkè®¤è¯
                    jqdatasdk.auth(username, password)
                    self.jqdata_initialized = True
                    logger.info("jqdatasdk åˆå§‹åŒ–æˆåŠŸ")
                else:
                    logger.warning("jqdatasdkå‡­è¯ä¸å®Œæ•´ï¼ŒjqdatasdkåŠŸèƒ½ä¸å¯ç”¨")

            except Exception as e:
                logger.error(f"jqdatasdkåˆå§‹åŒ–å¤±è´¥: {e}")
        elif not JQDATA_AVAILABLE:
            logger.warning("jqdatasdkåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install jqdatasdk")
        else:
            logger.info("jqdatasdkæ•°æ®æºæœªå¯ç”¨")

    def _init_tushare(self):
        """åˆå§‹åŒ–Tushare Pro API"""
        self.tushare_pro = None

        # æ£€æŸ¥æ˜¯å¦é…ç½®äº†Tushare
        if self.personal_config and self.personal_config.is_data_source_enabled('tushare'):
            try:
                import tushare as ts

                # è·å–Tushare token
                credentials = self.personal_config.get_api_credentials('tushare')
                token = credentials.get('token')

                if token:
                    # è®¾ç½®tokenå¹¶åˆå§‹åŒ–API
                    ts.set_token(token)
                    self.tushare_pro = ts.pro_api()
                    logger.info("Tushare Pro API åˆå§‹åŒ–æˆåŠŸ")
                else:
                    logger.warning("Tushare tokenæœªé…ç½®ï¼ŒTushareåŠŸèƒ½ä¸å¯ç”¨")

            except ImportError:
                logger.error("TushareåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install tushare")
            except Exception as e:
                logger.error(f"Tushareåˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            logger.info("Tushareæ•°æ®æºæœªå¯ç”¨")

    def _init_xtquant(self):
        """åˆå§‹åŒ–XTQuant"""
        self.xtquant_initialized = False

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†xtquant
        if self.personal_config and self.personal_config.is_data_source_enabled('xtquant') and XTQUANT_AVAILABLE:
            try:
                # xtquanté€šå¸¸ä¸éœ€è¦è®¤è¯ï¼Œä½†éœ€è¦MiniQmtæ”¯æŒ
                # å°è¯•è¿æ¥æµ‹è¯•
                test_result = xtdata.get_market_data_ex([], ["000001.SZ"], period="1d", count=1)
                self.xtquant_initialized = True
                logger.info("XTQuant åˆå§‹åŒ–æˆåŠŸ")

                # è®¾ç½®æœ¬åœ°æ•°æ®ç›®å½•
                if hasattr(xtdata, 'set_data_path'):
                    # å¯ä»¥è®¾ç½®è‡ªå®šä¹‰æ•°æ®è·¯å¾„
                    cache_dir = str(self.cache_dir / "xtquant_data")
                    xtdata.set_data_path(cache_dir)
                    logger.info(f"XTQuant æ•°æ®ç›®å½•è®¾ç½®ä¸º: {cache_dir}")

            except Exception as e:
                logger.error(f"XTQuantåˆå§‹åŒ–å¤±è´¥: {e}")
                logger.warning("è¯·ç¡®ä¿MiniQmtå·²æ­£ç¡®å®‰è£…å’Œé…ç½®")
        elif not XTQUANT_AVAILABLE:
            logger.warning("XTQuantåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install xtquant")
        else:
            logger.info("XTQuantæ•°æ®æºæœªå¯ç”¨")

    def fetch_benchmark_data(self, benchmark_symbol: str = "000300", start_date: str = None, end_date: str = None) -> pd.DataFrame:
        """
        è·å–åŸºå‡†æŒ‡æ•°æ•°æ®ç”¨äºBetaè®¡ç®—

        Args:
            benchmark_symbol: åŸºå‡†æŒ‡æ•°ä»£ç ï¼Œé»˜è®¤æ²ªæ·±300 (000300)
            start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼YYYYMMDD
            end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼YYYYMMDD

        Returns:
            åŒ…å«OHLCVæ•°æ®çš„DataFrame
        """
        if start_date is None:
            start_date = (datetime.now() - timedelta(days=3*365)).strftime('%Y%m%d')
        if end_date is None:
            end_date = datetime.now().strftime('%Y%m%d')

        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"benchmark_{benchmark_symbol}_{start_date}_{end_date}"
        cached_data = self._load_from_cache(cache_key)
        if cached_data is not None:
            logger.info(f"ä»ç¼“å­˜è·å–åŸºå‡†æ•°æ®: {benchmark_symbol}")
            return cached_data

        try:
            logger.info(f"è·å–åŸºå‡†æŒ‡æ•°æ•°æ®: {benchmark_symbol}, æ—¶é—´èŒƒå›´: {start_date} - {end_date}")

            # å°è¯•è·å–åŸºå‡†æ•°æ®
            data = self._try_benchmark_sources(benchmark_symbol, start_date, end_date)

            if data is not None and not data.empty:
                # ç¼“å­˜æ•°æ®
                self._save_to_cache(cache_key, data)
                logger.info(f"æˆåŠŸè·å–åŸºå‡†æ•°æ®: {benchmark_symbol}, æ•°æ®é‡: {len(data)}")
                return data
            else:
                logger.warning(f"æ— æ³•è·å–åŸºå‡†æ•°æ®: {benchmark_symbol}")
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"è·å–åŸºå‡†æ•°æ®å¤±è´¥ {benchmark_symbol}: {e}")
            return pd.DataFrame()

    def _try_benchmark_sources(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """å°è¯•å¤šç§æ•°æ®æºè·å–åŸºå‡†æ•°æ®"""
        logger.info(f"å¼€å§‹è·å–åŸºå‡†æ•°æ®: {symbol}, æ—¶é—´èŒƒå›´: {start_date} - {end_date}")

        # è·å–æŒ‰ä¼˜å…ˆçº§æ’åºçš„æ•°æ®æºåˆ—è¡¨
        enabled_sources = self._get_enabled_data_sources()
        logger.info(f"å¯ç”¨çš„æ•°æ®æºä¼˜å…ˆçº§: {enabled_sources}")

        # æ˜ å°„æ•°æ®æºåˆ°å¯¹åº”çš„åŸºå‡†è·å–æ–¹æ³•
        benchmark_methods = {
            'jqdata': lambda: self._try_jqdata_benchmark(symbol, start_date, end_date),
            'tushare': lambda: self._try_tushare_benchmark(symbol, start_date, end_date),
            'wind': lambda: self._try_wind_benchmark(symbol, start_date, end_date),
            'xtquant': lambda: self._try_xtquant_benchmark(symbol, start_date, end_date),
            'akshare': lambda: self._try_akshare_benchmark(symbol, start_date, end_date),
            'yfinance': lambda: self._try_yahoo_benchmark(symbol, start_date, end_date)
        }

        results = []

        # æŒ‰ä¼˜å…ˆçº§å°è¯•æ•°æ®æº
        for source_name in enabled_sources:
            if source_name in benchmark_methods:
                logger.info(f"ğŸ”„ æ­£åœ¨å°è¯•æ•°æ®æº: {source_name}")
                try:
                    data = benchmark_methods[source_name]()
                    if data is not None and not data.empty:
                        logger.info(f"âœ… {source_name} æˆåŠŸè·å–åŸºå‡†æ•°æ®: {len(data)} æ¡è®°å½•")
                        logger.info(f"   æ•°æ®å½¢çŠ¶: {data.shape}")
                        logger.info(f"   åˆ—å: {list(data.columns)}")
                        logger.info(f"   æ—¥æœŸèŒƒå›´: {data.index[0]} åˆ° {data.index[-1]}")

                        # éªŒè¯æ•°æ®è´¨é‡
                        if self._validate_benchmark_data_quality(data):
                            logger.info(f"âœ… {source_name} åŸºå‡†æ•°æ®è´¨é‡éªŒè¯é€šè¿‡")
                            return data
                        else:
                            logger.warning(f"âš ï¸ {source_name} åŸºå‡†æ•°æ®è´¨é‡éªŒè¯å¤±è´¥")
                            results.append((source_name, data, "è´¨é‡éªŒè¯å¤±è´¥"))
                    else:
                        logger.warning(f"âŒ {source_name} è¿”å›ç©ºæ•°æ®")
                        results.append((source_name, pd.DataFrame(), "è¿”å›ç©ºæ•°æ®"))
                except Exception as e:
                    logger.error(f"ğŸ’¥ {source_name} å¼‚å¸¸: {str(e)[:200]}")
                    logger.debug(f"   è¯¦ç»†é”™è¯¯ä¿¡æ¯: {type(e).__name__}: {e}")
                    results.append((source_name, pd.DataFrame(), f"å¼‚å¸¸: {type(e).__name__}"))

        # å¦‚æœæ‰€æœ‰é…ç½®çš„æ•°æ®æºéƒ½å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨çš„å…è´¹æ•°æ®æº
        if not any(result[2] == "è´¨é‡éªŒè¯é€šè¿‡" for result in results):
            logger.info("ğŸ”„ é…ç½®æ•°æ®æºå‡å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨å…è´¹æ•°æ®æº")
            fallback_sources = ['akshare', 'yfinance']
            for source_name in fallback_sources:
                if source_name not in enabled_sources and source_name in benchmark_methods:
                    logger.info(f"ğŸ”„ æ­£åœ¨å°è¯•å¤‡ç”¨æ•°æ®æº: {source_name}")
                    try:
                        data = benchmark_methods[source_name]()
                        if data is not None and not data.empty:
                            logger.info(f"âœ… å¤‡ç”¨æ•°æ®æº {source_name} æˆåŠŸè·å–æ•°æ®: {len(data)} æ¡è®°å½•")
                            logger.info(f"   æ•°æ®å½¢çŠ¶: {data.shape}")
                            logger.info(f"   åˆ—å: {list(data.columns)}")
                            logger.info(f"   æ—¥æœŸèŒƒå›´: {data.index[0]} åˆ° {data.index[-1]}")

                            if self._validate_benchmark_data_quality(data):
                                logger.info(f"âœ… å¤‡ç”¨æ•°æ®æº {source_name} è´¨é‡éªŒè¯é€šè¿‡")
                                return data
                            else:
                                logger.warning(f"âš ï¸ å¤‡ç”¨æ•°æ®æº {source_name} è´¨é‡éªŒè¯å¤±è´¥")
                                results.append((source_name, data, "è´¨é‡éªŒè¯å¤±è´¥"))
                        else:
                            logger.warning(f"âŒ å¤‡ç”¨æ•°æ®æº {source_name} è¿”å›ç©ºæ•°æ®")
                            results.append((source_name, pd.DataFrame(), "è¿”å›ç©ºæ•°æ®"))
                    except Exception as e:
                        logger.error(f"ğŸ’¥ å¤‡ç”¨æ•°æ®æº {source_name} å¼‚å¸¸: {str(e)[:200]}")
                        results.append((source_name, pd.DataFrame(), f"å¼‚å¸¸: {type(e).__name__}"))

        # è¾“å‡ºæ‰€æœ‰å°è¯•ç»“æœçš„è¯¦ç»†æŠ¥å‘Š
        logger.info("ğŸ“Š åŸºå‡†æ•°æ®è·å–å°è¯•ç»“æœæŠ¥å‘Š:")
        for source_name, data, status in results:
            if not data.empty:
                logger.info(f"  {source_name}: æˆåŠŸè·å– {len(data)} æ¡æ•°æ®, çŠ¶æ€: {status}")
            else:
                logger.warning(f"  {source_name}: å¤±è´¥, çŠ¶æ€: {status}")

        # å°è¯•æ¨¡æ‹Ÿæ•°æ®ä½œä¸ºæœ€åæ‰‹æ®µ
        if MOCK_GENERATOR_AVAILABLE and mock_generator.is_simulated_data_enabled():
            logger.info("ğŸ­ æ‰€æœ‰çœŸå®æ•°æ®æºå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®...")
            try:
                mock_data = mock_generator.generate_benchmark_data(symbol, start_date, end_date)
                if not mock_data.empty and self._validate_benchmark_data_quality(mock_data):
                    logger.info(f"ğŸ­ æ¨¡æ‹ŸåŸºå‡†æ•°æ®ç”ŸæˆæˆåŠŸ: {len(mock_data)} æ¡è®°å½•")
                    logger.warning("âš ï¸ æ³¨æ„: å½“å‰ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡ŒBetaè®¡ç®—ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œæ•°æ®æºé…ç½®")
                    return mock_data
                else:
                    logger.warning("ğŸ­ æ¨¡æ‹Ÿæ•°æ®éªŒè¯å¤±è´¥")
            except Exception as mock_e:
                logger.error(f"ğŸ­ æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå¤±è´¥: {type(mock_e).__name__}: {mock_e}")

        if not results:
            logger.error(f"âŒ æ‰€æœ‰æ•°æ®æºå°è¯•å‡æœªæ‰§è¡Œï¼Œå¯èƒ½æ˜¯é…ç½®é—®é¢˜")
            logger.info("ğŸ’¡ å»ºè®®:")
            logger.info("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            logger.info("   2. é…ç½®æœ‰æ•ˆçš„APIä»¤ç‰Œ")
            logger.info("   3. è®¾ç½® MOCK_DATA_ENABLED=true ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•")
        elif not any(result[2] in ["è´¨é‡éªŒè¯é€šè¿‡", "è¿”å›ç©ºæ•°æ®"] for result in results):
            logger.error(f"âŒ æ‰€æœ‰æ•°æ®æºå‡å¤±è´¥: {symbol}")
            logger.info("ğŸ’¡ å»ºè®®:")
            logger.info("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®")
            logger.info("   2. éªŒè¯APIä»¤ç‰Œæœ‰æ•ˆæ€§")
            logger.info("   3. è®¾ç½® MOCK_DATA_ENABLED=true ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•")
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„åŸºå‡†æ•°æ®: {symbol}")
            logger.info("ğŸ’¡ å»ºè®®è®¾ç½® MOCK_DATA_ENABLED=true ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•")

        return pd.DataFrame()

    def _try_akshare_benchmark(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """ä½¿ç”¨AKShareè·å–åŸºå‡†æ•°æ®"""
        try:
            # è½¬æ¢æ—¥æœŸæ ¼å¼ä¸ºYYYYMMDD
            start_clean = start_date.replace('-', '')
            end_clean = end_date.replace('-', '')

            # å¯¹äºæŒ‡æ•°ä»£ç ï¼Œå°è¯•å¤šç§AKShareæ–¹æ³•
            ak_methods = []

            if symbol == "000300":
                # æ²ªæ·±300æŒ‡æ•°çš„å¤šç§è·å–æ–¹å¼
                ak_methods = [
                    # æ–¹æ³•1: ä½¿ç”¨stock_zh_index_daily_em (æ¨è)
                    lambda: ak.stock_zh_index_daily_em(symbol="000300", start_date=start_clean, end_date=end_clean),
                    # æ–¹æ³•2: ä½¿ç”¨index_zh_a_hist
                    lambda: ak.index_zh_a_hist(symbol="000300", period="daily", start_date=start_date, end_date=end_date),
                    # æ–¹æ³•3: ç›´æ¥ä½¿ç”¨æŒ‡æ•°ä»£ç 
                    lambda: ak.stock_zh_a_hist(symbol="000300", period="daily", start_date=start_clean, end_date=end_clean),
                    # æ–¹æ³•4: ä½¿ç”¨å¸¦å‰ç¼€çš„æ ¼å¼
                    lambda: ak.stock_zh_a_hist(symbol="sh000300", period="daily", start_date=start_clean, end_date=end_clean)
                ]
            else:
                # å…¶ä»–æŒ‡æ•°çš„å¤„ç†
                ak_methods = [
                    lambda: ak.stock_zh_a_hist(symbol=symbol, period="daily", start_date=start_clean, end_date=end_clean),
                    lambda: ak.stock_zh_a_hist(symbol=f"sh{symbol}", period="daily", start_date=start_clean, end_date=end_clean)
                ]

            for i, method in enumerate(ak_methods):
                try:
                    logger.debug(f"AKShareåŸºå‡†æ•°æ® - å°è¯•æ–¹æ³• {i+1}/{len(ak_methods)}")
                    data = method()

                    if data is not None and not data.empty:
                        logger.info(f"AKShareåŸºå‡†æ•°æ® - æ–¹æ³• {i+1} æˆåŠŸè·å– {len(data)} æ¡æ•°æ®")

                        # æ ‡å‡†åŒ–åˆ—åæ˜ å°„
                        column_mapping = {
                            'æ—¥æœŸ': 'date',
                            'å¼€ç›˜': 'open',
                            'æ”¶ç›˜': 'close',
                            'æœ€é«˜': 'high',
                            'æœ€ä½': 'low',
                            'æˆäº¤é‡': 'volume',
                            'æˆäº¤é¢': 'amount',
                            'æ¶¨è·Œå¹…': 'change_pct'
                        }

                        # åº”ç”¨åˆ—åæ˜ å°„
                        data = data.rename(columns={k: v for k, v in column_mapping.items() if k in data.columns})

                        # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
                        if 'date' not in data.columns:
                            # å°è¯•å…¶ä»–å¯èƒ½çš„æ—¥æœŸåˆ—å
                            for date_col in ['time', 'datetime', 'æ—¥æœŸ']:
                                if date_col in data.columns:
                                    data = data.rename(columns={date_col: 'date'})
                                    break

                        if 'date' in data.columns:
                            data['date'] = pd.to_datetime(data['date'])
                            data = data.sort_values('date').reset_index(drop=True)
                        else:
                            logger.warning(f"AKShareæ•°æ®æœªæ‰¾åˆ°æ—¥æœŸåˆ—ï¼Œç°æœ‰åˆ—: {list(data.columns)}")
                            continue

                        # ç¡®ä¿æœ‰ä»·æ ¼æ•°æ®
                        if 'close' not in data.columns:
                            # å°è¯•å…¶ä»–ä»·æ ¼åˆ—å
                            for price_col in ['æ”¶ç›˜ä»·', 'Close', 'æ”¶ç›˜']:
                                if price_col in data.columns:
                                    data = data.rename(columns={price_col: 'close'})
                                    break

                        if 'close' in data.columns:
                            # è½¬æ¢ä»·æ ¼æ•°æ®ä¸ºæ•°å€¼ç±»å‹
                            data['close'] = pd.to_numeric(data['close'], errors='coerce')
                            return data
                        else:
                            logger.warning(f"AKShareæ•°æ®æœªæ‰¾åˆ°ä»·æ ¼æ•°æ®ï¼Œç°æœ‰åˆ—: {list(data.columns)}")
                            continue

                except Exception as method_e:
                    logger.debug(f"AKShareåŸºå‡†æ•°æ® - æ–¹æ³• {i+1} å¤±è´¥: {type(method_e).__name__}: {method_e}")
                    continue

        except Exception as e:
            logger.warning(f"AKShareåŸºå‡†æ•°æ®è·å–å®Œå…¨å¤±è´¥: {type(e).__name__}: {e}")

        logger.warning("AKShare: æ‰€æœ‰åŸºå‡†æ•°æ®è·å–æ–¹æ³•å‡å¤±è´¥")
        return pd.DataFrame()

    def _try_yahoo_benchmark(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """ä½¿ç”¨Yahoo Financeè·å–åŸºå‡†æ•°æ®"""
        try:
            import yfinance as yf

            # è½¬æ¢æ—¥æœŸæ ¼å¼
            start_clean = start_date.replace('-', '')
            end_clean = end_date.replace('-', '')
            start_dt = datetime.strptime(start_clean, '%Y%m%d').strftime('%Y-%m-%d')
            end_dt = datetime.strptime(end_clean, '%Y%m%d').strftime('%Y-%m-%d')

            # å°è¯•å¤šç§ç¬¦å·æ ¼å¼
            symbol_formats = []

            if symbol == "000300":
                # æ²ªæ·±300çš„å¤šç§æ ¼å¼
                symbol_formats = [
                    "000300.SS",     # æ ‡å‡†ä¸Šæµ·æ ¼å¼
                    "000300.SZ",     # æ·±åœ³æ ¼å¼
                    "^HSI",          # æ’ç”ŸæŒ‡æ•°ä½œä¸ºæ›¿ä»£
                    "300750.SZ",     # å®å¾·æ—¶ä»£ä½œä¸ºæ´»è·ƒè‚¡ç¥¨æµ‹è¯•
                ]
            elif symbol.startswith("00"):
                # æ·±åœ³å¸‚åœºä»£ç 
                symbol_formats = [f"{symbol}.SZ"]
            else:
                # ä¸Šæµ·å¸‚åœºä»£ç 
                symbol_formats = [f"{symbol}.SS"]

            # æ·»åŠ ä¸€äº›å›½é™…æŒ‡æ•°ä½œä¸ºç½‘ç»œè¿æ¥æµ‹è¯•
            test_symbols = ["^GSPC", "SPY", "QQQ", "VTI"]
            symbol_formats.extend(test_symbols)

            logger.debug(f"Yahoo FinanceåŸºå‡†æ•°æ® - å°è¯• {len(symbol_formats)} ç§ç¬¦å·æ ¼å¼")

            for i, yf_symbol in enumerate(symbol_formats):
                try:
                    logger.debug(f"Yahoo FinanceåŸºå‡†æ•°æ® - æµ‹è¯•ç¬¦å· {i+1}/{len(symbol_formats)}: {yf_symbol}")

                    # è®¾ç½®è¶…æ—¶å’Œé‡è¯•å‚æ•°
                    data = yf.download(
                        yf_symbol,
                        start=start_dt,
                        end=end_dt,
                        progress=False,
                        timeout=15,
                        show_errors=False
                    )

                    if data is not None and not data.empty:
                        logger.info(f"Yahoo FinanceåŸºå‡†æ•°æ® - ç¬¦å· {yf_symbol} æˆåŠŸè·å– {len(data)} æ¡æ•°æ®")

                        # é‡ç½®ç´¢å¼•ä»¥è·å–æ—¥æœŸåˆ—
                        data = data.reset_index()

                        # æ ‡å‡†åŒ–åˆ—å
                        column_mapping = {
                            'Date': 'date',
                            'Open': 'open',
                            'High': 'high',
                            'Low': 'low',
                            'Close': 'close',
                            'Volume': 'volume',
                            'Adj Close': 'adj_close'
                        }

                        data = data.rename(columns={k: v for k, v in column_mapping.items() if k in data.columns})

                        # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
                        if 'date' in data.columns:
                            data['date'] = pd.to_datetime(data['date'])
                            data = data.sort_values('date').reset_index(drop=True)

                        # ç¡®ä¿ä»·æ ¼æ•°æ®ä¸ºæ•°å€¼ç±»å‹
                        numeric_columns = ['open', 'high', 'low', 'close', 'volume']
                        for col in numeric_columns:
                            if col in data.columns:
                                data[col] = pd.to_numeric(data[col], errors='coerce')

                        logger.debug(f"Yahoo Financeæ•°æ®åˆ—: {list(data.columns)}")
                        return data
                    else:
                        logger.debug(f"Yahoo Financeç¬¦å· {yf_symbol} è¿”å›ç©ºæ•°æ®")

                except Exception as symbol_e:
                    logger.debug(f"Yahoo Financeç¬¦å· {yf_symbol} å¤±è´¥: {type(symbol_e).__name__}: {symbol_e}")
                    continue

        except ImportError:
            logger.warning("Yahoo Finance: yfinanceåº“æœªå®‰è£…")
            return pd.DataFrame()
        except Exception as e:
            logger.warning(f"Yahoo FinanceåŸºå‡†æ•°æ®è·å–å®Œå…¨å¤±è´¥: {type(e).__name__}: {e}")

        logger.warning("Yahoo Finance: æ‰€æœ‰ç¬¦å·æ ¼å¼å‡å¤±è´¥")
        return pd.DataFrame()

    def _try_tushare_benchmark(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """ä½¿ç”¨Tushare Pro APIè·å–åŸºå‡†æ•°æ®"""
        if self.tushare_pro is None:
            logger.debug("Tushare Proæœªåˆå§‹åŒ–")
            return pd.DataFrame()

        try:
            # è½¬æ¢æŒ‡æ•°ä»£ç ä¸ºTushareæ ¼å¼
            ts_symbol = self._convert_benchmark_to_tushare(symbol)

            # è·å–æŒ‡æ•°æ—¥çº¿æ•°æ®
            df = self.tushare_pro.index_daily(
                ts_code=ts_symbol,
                start_date=start_date,
                end_date=end_date,
                fields='ts_code,trade_date,open,high,low,close,vol,amount'
            )

            if df.empty:
                logger.debug(f"Tushareæœªè·å–åˆ°åŸºå‡†æ•°æ®: {ts_symbol}")
                return pd.DataFrame()

            # æ•°æ®æ¸…æ´—å’Œæ ¼å¼è½¬æ¢
            df = df.sort_values('trade_date')
            df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
            df = df.set_index('trade_date')

            # é‡å‘½ååˆ—ä»¥åŒ¹é…æ ‡å‡†æ ¼å¼
            df = df.rename(columns={
                'open': 'open',
                'high': 'high',
                'low': 'low',
                'close': 'close',
                'vol': 'volume',
                'amount': 'amount'
            })

            # é€‰æ‹©éœ€è¦çš„åˆ—
            df = df[['open', 'high', 'low', 'close', 'volume']]

            logger.info(f"TushareæˆåŠŸè·å–åŸºå‡†æ•°æ® {len(df)} æ¡: {symbol}")
            return df

        except Exception as e:
            logger.debug(f"Tushareè·å–åŸºå‡†æ•°æ®å¤±è´¥ {symbol}: {e}")
            return pd.DataFrame()

    def _convert_benchmark_to_tushare(self, symbol: str) -> str:
        """è½¬æ¢æŒ‡æ•°ä»£ç ä¸ºTushareæ ¼å¼"""
        # æ²ªæ·±300æŒ‡æ•°
        if symbol == "000300":
            return "000300.SH"
        # ä¸Šè¯ç»¼æŒ‡
        elif symbol == "000001":
            return "000001.SH"
        # ä¸­è¯500æŒ‡æ•°
        elif symbol == "000905":
            return "000905.SH"
        # æ·±è¯æˆæŒ‡
        elif symbol == "399001":
            return "399001.SZ"
        # åˆ›ä¸šæ¿æŒ‡
        elif symbol == "399006":
            return "399006.SZ"
        else:
            # é»˜è®¤å°è¯•ä¸Šæµ·å¸‚åœº
            return f"{symbol}.SH"

    def _try_wind_benchmark(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """ä½¿ç”¨Wind APIè·å–åŸºå‡†æ•°æ®ï¼ˆé¢„ç•™æ¥å£ï¼‰"""
        # Windé›†æˆéœ€è¦ç‰¹æ®Šçš„æˆæƒå’Œå®‰è£…ï¼Œè¿™é‡Œæä¾›åŸºç¡€æ¡†æ¶
        logger.debug("WindåŸºå‡†æ•°æ®æ¥å£æš‚æœªå®ç°")
        return pd.DataFrame()

    def fetch_etf_data(self, symbol: str, start_date: str = None, end_date: str = None) -> pd.DataFrame:
        """
        è·å–ETFå†å²æ•°æ®

        Args:
            symbol: ETFä»£ç 
            start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼YYYYMMDD
            end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼YYYYMMDD

        Returns:
            åŒ…å«OHLCVæ•°æ®çš„DataFrame
        """
        if start_date is None:
            start_date = (datetime.now() - timedelta(days=3*365)).strftime('%Y%m%d')
        if end_date is None:
            end_date = datetime.now().strftime('%Y%m%d')

        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"etf_{symbol}_{start_date}_{end_date}"
        cached_data = self._load_from_cache(cache_key)
        if cached_data is not None:
            logger.info(f"ä»ç¼“å­˜è·å–æ•°æ®: {symbol}")
            return cached_data

        try:
            logger.info(f"è·å–ETFæ•°æ®: {symbol}, æ—¶é—´èŒƒå›´: {start_date} - {end_date}")

            # å°è¯•å¤šç§æ•°æ®æº
            data = self._try_data_sources(symbol, start_date, end_date)

            if data.empty:
                logger.warning(f"æœªè·å–åˆ°æ•°æ®: {symbol}")
                return pd.DataFrame()

            # æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
            data = self._clean_and_standardize_data(data)

            # ä¿å­˜åˆ°ç¼“å­˜
            self._save_to_cache(cache_key, data)

            logger.info(f"æˆåŠŸè·å– {len(data)} æ¡æ•°æ®: {symbol}")
            return data

        except Exception as e:
            logger.error(f"è·å–ETFæ•°æ®å¤±è´¥ {symbol}: {e}")
            return pd.DataFrame()

    def _try_data_sources(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """å°è¯•å¤šç§æ•°æ®æºï¼Œæé«˜æ•°æ®è·å–æˆåŠŸç‡"""
        # è·å–æŒ‰ä¼˜å…ˆçº§æ’åºçš„æ•°æ®æºåˆ—è¡¨
        enabled_sources = self._get_enabled_data_sources()

        # æ˜ å°„æ•°æ®æºåˆ°å¯¹åº”çš„æ–¹æ³•
        source_methods = {
            'jqdata': self._try_jqdata,
            'tushare': self._try_tushare,
            'wind': self._try_wind,  # é¢„ç•™Windæ¥å£
            'xtquant': self._try_xtquant,
            'akshare': self._try_akshare_primary,
            'yfinance': self._try_yfinance
        }

        # æŒ‰ä¼˜å…ˆçº§å°è¯•æ•°æ®æº
        for source_name in enabled_sources:
            if source_name in source_methods:
                source_func = source_methods[source_name]
                try:
                    logger.info(f"å°è¯•æ•°æ®æº: {source_name}")
                    data = source_func(symbol, start_date, end_date)
                    if not data.empty:
                        logger.info(f"ä½¿ç”¨ {source_name} æˆåŠŸè·å–æ•°æ®")
                        return data
                    else:
                        logger.debug(f"{source_name} è¿”å›ç©ºæ•°æ®")
                except Exception as e:
                    logger.debug(f"{source_name} å¤±è´¥: {str(e)[:100]}")
            else:
                logger.debug(f"æœªçŸ¥æ•°æ®æº: {source_name}")

        # å¦‚æœæ‰€æœ‰é…ç½®çš„æ•°æ®æºéƒ½å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨çš„å…è´¹æ•°æ®æº
        fallback_sources = ['akshare', 'yfinance']
        for source_name in fallback_sources:
            if source_name not in enabled_sources and source_name in source_methods:
                logger.info(f"å°è¯•å¤‡ç”¨æ•°æ®æº: {source_name}")
                try:
                    data = source_methods[source_name](symbol, start_date, end_date)
                    if not data.empty:
                        logger.info(f"ä½¿ç”¨å¤‡ç”¨æ•°æ®æº {source_name} æˆåŠŸè·å–æ•°æ®")
                        return data
                except Exception as e:
                    logger.debug(f"å¤‡ç”¨æ•°æ®æº {source_name} å¤±è´¥: {str(e)[:100]}")

        logger.warning(f"æ‰€æœ‰æ•°æ®æºå‡å¤±è´¥: {symbol}")
        return pd.DataFrame()

    def _get_enabled_data_sources(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ•°æ®æºåˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº"""
        if self.personal_config:
            try:
                return self.personal_config.get_enabled_data_sources()
            except Exception as e:
                logger.error(f"è·å–å¯ç”¨æ•°æ®æºå¤±è´¥: {e}")

        # å›é€€åˆ°é»˜è®¤æ•°æ®æº
        return ['akshare', 'yfinance']

    def _try_akshare_primary(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """ä¸»è¦akshareæ•°æ®æº - ä¼˜å…ˆä½¿ç”¨å¯ç”¨çš„æ–¹æ³•"""

        # æ–¹æ³•1: å°è¯• fund_etf_hist_sina (è¿™ä¸ªæ–¹æ³•å·¥ä½œæ­£å¸¸)
        try:
            # è½¬æ¢ä»£ç æ ¼å¼
            if symbol.startswith('51'):
                sina_symbol = f"sh{symbol}"  # ä¸Šæµ·ETF
            elif symbol.startswith('15'):
                sina_symbol = f"sz{symbol}"  # æ·±åœ³ETF
            else:
                sina_symbol = symbol

            logger.debug(f"å°è¯•AKShareæ–°æµªæ–¹æ³•: {sina_symbol}")
            data = ak.fund_etf_hist_sina(symbol=sina_symbol)

            if data is not None and not data.empty:
                # è¿‡æ»¤æ—¥æœŸèŒƒå›´
                data['date'] = pd.to_datetime(data['date'])
                start_dt = pd.to_datetime(start_date, format='%Y%m%d')
                end_dt = pd.to_datetime(end_date, format='%Y%m%d')

                # è¿‡æ»¤æ—¥æœŸ
                filtered_data = data[
                    (data['date'] >= start_dt) & (data['date'] <= end_dt)
                ].copy()

                # ç¡®ä¿åˆ—åæ ‡å‡†åŒ–
                if all(col in filtered_data.columns for col in ['date', 'open', 'high', 'low', 'close', 'volume']):
                    logger.info(f"AKShareæ–°æµªæ–¹æ³•æˆåŠŸè·å– {len(filtered_data)} æ¡æ•°æ®")
                    return filtered_data
                else:
                    logger.debug(f"AKShareæ–°æµªæ–¹æ³•æ•°æ®åˆ—ä¸å®Œæ•´: {filtered_data.columns.tolist()}")

        except Exception as e:
            logger.debug(f"AKShareæ–°æµªæ–¹æ³•å¤±è´¥: {e}")

        # æ–¹æ³•2: å°è¯• stock_zh_a_hist (å¤‡ç”¨æ–¹æ³•)
        try:
            logger.debug(f"å°è¯•AKShareè‚¡ç¥¨å†å²æ•°æ®æ–¹æ³•: {symbol}")
            data = ak.stock_zh_a_hist(
                symbol=symbol,
                period="daily",
                start_date=start_date,
                end_date=end_date,
                adjust="hfq"
            )

            if data is not None and not data.empty:
                # æ ‡å‡†åŒ–åˆ—åæ˜ å°„
                column_mapping = {
                    'æ—¥æœŸ': 'date',
                    'å¼€ç›˜': 'open',
                    'æœ€é«˜': 'high',
                    'æœ€ä½': 'low',
                    'æ”¶ç›˜': 'close',
                    'æˆäº¤é‡': 'volume',
                    'æˆäº¤é¢': 'amount'
                }

                standardized_data = pd.DataFrame()
                for old_name, new_name in column_mapping.items():
                    if old_name in data.columns:
                        standardized_data[new_name] = data[old_name]

                # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
                if all(col in standardized_data.columns for col in ['date', 'open', 'high', 'low', 'close', 'volume']):
                    logger.info(f"AKShareè‚¡ç¥¨å†å²æ–¹æ³•æˆåŠŸè·å– {len(standardized_data)} æ¡æ•°æ®")
                    return standardized_data

        except Exception as e:
            logger.debug(f"AKShareè‚¡ç¥¨å†å²æ–¹æ³•å¤±è´¥: {e}")

        # æ–¹æ³•3: å°è¯• fund_etf_hist_em (è¿™ä¸ªæ–¹æ³•æœ‰ä»£ç†é—®é¢˜ï¼Œä½†æœ€åå°è¯•)
        try:
            logger.debug(f"å°è¯•AKShareä¸œæ–¹è´¢å¯Œæ–¹æ³•: {symbol}")
            data = ak.fund_etf_hist_em(
                symbol=symbol,
                period="daily",
                start_date=start_date,
                end_date=end_date,
                adjust="hfq"
            )
            if data is not None and not data.empty:
                logger.info(f"AKShareä¸œæ–¹è´¢å¯Œæ–¹æ³•æˆåŠŸè·å– {len(data)} æ¡æ•°æ®")
                return data

        except Exception as e:
            logger.debug(f"AKShareä¸œæ–¹è´¢å¯Œæ–¹æ³•å¤±è´¥: {e}")

        logger.debug(f"æ‰€æœ‰AKShareæ–¹æ³•éƒ½å¤±è´¥äº†: {symbol}")
        return pd.DataFrame()

    def _try_akshare_secondary(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """å¤‡é€‰akshareæ•°æ®æº - fund_etf_hist_sina"""
        try:
            # è½¬æ¢ä»£ç æ ¼å¼
            if symbol.startswith('51'):
                sina_symbol = f"sh{symbol}"  # ä¸Šæµ·ETF
            elif symbol.startswith('15'):
                sina_symbol = f"sz{symbol}"  # æ·±åœ³ETF
            else:
                sina_symbol = symbol

            # è·å–æ•°æ®
            data = ak.fund_etf_hist_sina(symbol=sina_symbol)

            if not data.empty:
                # è½¬æ¢æ—¥æœŸæ ¼å¼
                data['date'] = pd.to_datetime(data['date'])

                # è¿‡æ»¤æ—¥æœŸèŒƒå›´
                if start_date:
                    start_dt = pd.to_datetime(start_date)
                    data = data[data['date'] >= start_dt]
                if end_date:
                    end_dt = pd.to_datetime(end_date)
                    data = data[data['date'] <= end_dt]

                return data
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.debug(f"akshare secondaryå¤±è´¥: {e}")
            return pd.DataFrame()

    def _try_tushare(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """ä½¿ç”¨Tushare Pro APIè·å–æ•°æ®"""
        if self.tushare_pro is None:
            logger.debug("Tushare Proæœªåˆå§‹åŒ–")
            return pd.DataFrame()

        try:
            # Tushareéœ€è¦ç‰¹å®šçš„è‚¡ç¥¨ä»£ç æ ¼å¼
            ts_symbol = self._convert_to_tushare_symbol(symbol)

            # è·å–åŸºç¡€ä¿¡æ¯ç¡®å®šè‚¡ç¥¨ç±»å‹
            basic_info = self.tushare_pro.basic(
                ts_code=ts_symbol,
                fields='ts_code,name,area,industry,list_date'
            )

            if basic_info.empty:
                logger.debug(f"Tushareæœªæ‰¾åˆ°è‚¡ç¥¨ä»£ç : {ts_symbol}")
                return pd.DataFrame()

            # æ ¹æ®è‚¡ç¥¨ç±»å‹è·å–æ—¥çº¿æ•°æ®
            if ts_symbol.endswith('.SH') or ts_symbol.endswith('.SZ'):
                # Aè‚¡æˆ–ETF
                df = self.tushare_pro.daily(
                    ts_code=ts_symbol,
                    start_date=start_date,
                    end_date=end_date,
                    fields='ts_code,trade_date,open,high,low,close,pre_close,vol,amount'
                )
            else:
                logger.debug(f"ä¸æ”¯æŒçš„è‚¡ç¥¨ä»£ç æ ¼å¼: {ts_symbol}")
                return pd.DataFrame()

            if df.empty:
                logger.debug(f"Tushareæœªè·å–åˆ°æ•°æ®: {ts_symbol}")
                return pd.DataFrame()

            # æ•°æ®æ¸…æ´—å’Œæ ¼å¼è½¬æ¢
            df = df.sort_values('trade_date')
            df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
            df = df.set_index('trade_date')

            # é‡å‘½ååˆ—ä»¥åŒ¹é…æ ‡å‡†æ ¼å¼
            df = df.rename(columns={
                'open': 'open',
                'high': 'high',
                'low': 'low',
                'close': 'close',
                'vol': 'volume',
                'amount': 'amount'
            })

            # é€‰æ‹©éœ€è¦çš„åˆ—
            df = df[['open', 'high', 'low', 'close', 'volume']]

            logger.info(f"TushareæˆåŠŸè·å– {len(df)} æ¡æ•°æ®: {symbol}")
            return df

        except Exception as e:
            logger.debug(f"Tushareè·å–æ•°æ®å¤±è´¥ {symbol}: {e}")
            return pd.DataFrame()

    def _convert_to_tushare_symbol(self, symbol: str) -> str:
        """è½¬æ¢ETFä»£ç ä¸ºTushareæ ¼å¼"""
        # ç§»é™¤å¯èƒ½çš„å‰ç¼€å’Œåç¼€
        clean_symbol = symbol.replace('etf', '').replace('ETF', '').replace('SH', '').replace('SZ', '')

        # ç¡®å®šå¸‚åœºåç¼€
        if symbol.startswith(('51', '58', '56')):  # ä¸Šæµ·å¸‚åœºETF
            return f"{clean_symbol}.SH"
        elif symbol.startswith(('15', '16', '159')):  # æ·±åœ³å¸‚åœºETF
            return f"{clean_symbol}.SZ"
        else:
            # é»˜è®¤å°è¯•ä¸Šæµ·å¸‚åœº
            return f"{clean_symbol}.SH"

    def _convert_to_jqdata_symbol(self, symbol: str) -> str:
        """è½¬æ¢è¯åˆ¸ä»£ç ä¸ºjqdatasdkæ ¼å¼"""
        # ç§»é™¤å¯èƒ½çš„å‰ç¼€å’Œåç¼€
        clean_symbol = symbol.replace('etf', '').replace('ETF', '').replace('XSHG', '').replace('XSHE', '').replace('SH', '').replace('SZ', '')

        # ETFä»£ç è½¬æ¢
        if symbol.startswith(('51', '58', '56')):  # ä¸Šæµ·ETF
            return f"{clean_symbol}.XSHG"
        elif symbol.startswith(('15', '16', '159')):  # æ·±åœ³ETF
            return f"{clean_symbol}.XSHE"
        # æŒ‡æ•°ä»£ç è½¬æ¢
        elif symbol.startswith('000'):  # æ·±åœ³æŒ‡æ•°ï¼ˆå¦‚000001.SZï¼‰
            return f"{clean_symbol}.XSHE"
        elif symbol.startswith(('399', '0009')):  # æ·±è¯æˆæŒ‡ç­‰
            return f"{symbol}.XSHE"
        elif symbol.startswith(('0003', '0009')):  # æ²ªæ·±300ç­‰ç‰¹æ®ŠæŒ‡æ•°
            return f"{symbol}.XSHG"
        # ä¸ªè‚¡ä»£ç è½¬æ¢
        elif symbol.startswith(('600', '601', '603', '605')):  # ä¸Šæµ·Aè‚¡
            return f"{clean_symbol}.XSHG"
        elif symbol.startswith(('000', '001', '002', '003')):  # æ·±åœ³Aè‚¡
            return f"{clean_symbol}.XSHE"
        else:
            # é»˜è®¤å¤„ç†ä¸ºä¸Šæµ·å¸‚åœº
            return f"{clean_symbol}.XSHG"

    def _try_jqdata(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """ä½¿ç”¨jqdatasdkè·å–ETFå’Œä¸ªè‚¡æ•°æ®"""
        if not self.jqdata_initialized:
            logger.debug("jqdatasdkæœªåˆå§‹åŒ–")
            return pd.DataFrame()

        try:
            # è½¬æ¢æ—¥æœŸæ ¼å¼
            start_dt = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')

            # è½¬æ¢è¯åˆ¸ä»£ç ä¸ºjqdatasdkæ ¼å¼
            jq_symbol = self._convert_to_jqdata_symbol(symbol)

            # è·å–å†å²æ•°æ®
            data = get_price(
                security=jq_symbol,
                start_date=start_dt.strftime('%Y-%m-%d'),
                end_date=end_dt.strftime('%Y-%m-%d'),
                frequency='daily'
            )

            if data is None or data.empty:
                logger.debug(f"jqdatasdkæœªè·å–åˆ°æ•°æ®: {jq_symbol}")
                return pd.DataFrame()

            # æ ‡å‡†åŒ–åˆ—åå’Œæ•°æ®æ ¼å¼
            data = data.reset_index()

            # é‡å‘½ååˆ—ä»¥åŒ¹é…æ ‡å‡†æ ¼å¼
            column_mapping = {
                'open': 'open',
                'high': 'high',
                'low': 'low',
                'close': 'close',
                'volume': 'volume'
            }

            # åº”ç”¨åˆ—åæ˜ å°„
            for old_col, new_col in column_mapping.items():
                if old_col in data.columns:
                    data = data.rename(columns={old_col: new_col})

            # ç¡®ä¿æ—¥æœŸåˆ—æ ¼å¼æ­£ç¡®
            if 'date' not in data.columns and 'index' in str(data.index.names):
                data['date'] = data.index
                data = data.reset_index(drop=True)

            if 'date' in data.columns:
                data['date'] = pd.to_datetime(data['date'])
                data = data.sort_values('date').reset_index(drop=True)

            # é€‰æ‹©éœ€è¦çš„åˆ—
            required_columns = ['date', 'open', 'high', 'low', 'close']
            if 'volume' in data.columns:
                required_columns.append('volume')
            else:
                data['volume'] = 0

            # ç¡®ä¿æ‰€æœ‰å¿…è¦åˆ—éƒ½å­˜åœ¨
            for col in ['open', 'high', 'low', 'close']:
                if col not in data.columns:
                    logger.warning(f"jqdatasdkæ•°æ®ç¼ºå°‘åˆ—: {col}")
                    data[col] = data['close']  # ä½¿ç”¨æ”¶ç›˜ä»·å¡«å……ç¼ºå¤±çš„ä»·æ ¼æ•°æ®

            logger.info(f"jqdatasdkæˆåŠŸè·å–æ•°æ® {len(data)} æ¡: {symbol}")
            return data[required_columns]

        except Exception as e:
            logger.debug(f"jqdatasdkè·å–æ•°æ®å¤±è´¥ {symbol}: {e}")
            return pd.DataFrame()

    def _try_jqdata_benchmark(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """ä½¿ç”¨jqdatasdkè·å–åŸºå‡†æ•°æ®"""
        if not self.jqdata_initialized:
            logger.debug("jqdatasdkæœªåˆå§‹åŒ–")
            return pd.DataFrame()

        try:
            # è½¬æ¢æ—¥æœŸæ ¼å¼
            start_dt = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')

            # è½¬æ¢æŒ‡æ•°ä»£ç ä¸ºjqdatasdkæ ¼å¼
            jq_symbol = self._convert_to_jqdata_symbol(symbol)

            # è·å–æŒ‡æ•°å†å²æ•°æ®
            data = get_price(
                security=jq_symbol,
                start_date=start_dt.strftime('%Y-%m-%d'),
                end_date=end_dt.strftime('%Y-%m-%d'),
                frequency='daily'
            )

            if data is None or data.empty:
                logger.debug(f"jqdatasdkæœªè·å–åˆ°åŸºå‡†æ•°æ®: {jq_symbol}")
                return pd.DataFrame()

            # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
            data = data.reset_index()

            # é‡å‘½ååˆ—ä»¥åŒ¹é…æ ‡å‡†æ ¼å¼
            column_mapping = {
                'open': 'open',
                'high': 'high',
                'low': 'low',
                'close': 'close',
                'volume': 'volume'
            }

            # åº”ç”¨åˆ—åæ˜ å°„
            for old_col, new_col in column_mapping.items():
                if old_col in data.columns:
                    data = data.rename(columns={old_col: new_col})

            # ç¡®ä¿æ—¥æœŸåˆ—æ ¼å¼æ­£ç¡®
            if 'date' not in data.columns:
                data['date'] = data.index
                data = data.reset_index(drop=True)

            if 'date' in data.columns:
                data['date'] = pd.to_datetime(data['date'])
                data = data.sort_values('date').reset_index(drop=True)

            # ç¡®ä¿æˆäº¤é‡åˆ—å­˜åœ¨
            if 'volume' not in data.columns:
                data['volume'] = 0

            # é€‰æ‹©éœ€è¦çš„åˆ—
            required_columns = ['date', 'open', 'high', 'low', 'close', 'volume']

            logger.info(f"jqdatasdkæˆåŠŸè·å–åŸºå‡†æ•°æ® {len(data)} æ¡: {symbol}")
            return data[required_columns]

        except Exception as e:
            logger.debug(f"jqdatasdkè·å–åŸºå‡†æ•°æ®å¤±è´¥ {symbol}: {e}")
            return pd.DataFrame()

    def _try_xtquant_benchmark(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """ä½¿ç”¨XTQuantè·å–åŸºå‡†æŒ‡æ•°æ•°æ®"""
        if not self.xtquant_initialized or not XTQUANT_AVAILABLE:
            logger.debug("XTQuantæœªåˆå§‹åŒ–æˆ–ä¸å¯ç”¨")
            return pd.DataFrame()

        try:
            logger.info(f"XTQuant: å¼€å§‹è·å–åŸºå‡†æŒ‡æ•°æ•°æ® {symbol}")

            # è½¬æ¢æ—¥æœŸæ ¼å¼
            start_dt = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')

            # è½¬æ¢æŒ‡æ•°ä»£ç ä¸ºXTQuantæ ¼å¼
            xt_symbol = self._convert_to_xtquant_symbol(symbol)
            logger.debug(f"XTQuant: è½¬æ¢ååŸºå‡†ä»£ç  {xt_symbol}")

            # é¦–å…ˆå°è¯•è·å–æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™å…ˆä¸‹è½½
            data = xtdata.get_market_data_ex([], [xt_symbol], period="1d", count=-1)

            if data is None or not data or xt_symbol not in data:
                logger.info(f"XTQuant: æœ¬åœ°æ— åŸºå‡†æ•°æ®ï¼Œå¼€å§‹ä¸‹è½½ {xt_symbol}")
                # ä¸‹è½½åŸºå‡†æŒ‡æ•°å†å²æ•°æ®
                xtdata.download_history_data(xt_symbol, period="1d", incrementally=True)
                # å†æ¬¡è·å–æ•°æ®
                data = xtdata.get_market_data_ex([], [xt_symbol], period="1d", count=-1)

            if not data or xt_symbol not in data:
                logger.warning(f"XTQuant: ä¸‹è½½åä»æ— æ³•è·å–åŸºå‡†æ•°æ® {xt_symbol}")
                return pd.DataFrame()

            # å¤„ç†XTQuantè¿”å›çš„æ•°æ®æ ¼å¼
            df = data[xt_symbol]

            # å°†æ•°æ®è½¬æ¢ä¸ºDataFrame
            ohlc_data = []
            for timestamp, row_data in df.items():
                # timestampæ˜¯æ—¶é—´æˆ³æ ¼å¼
                date = pd.to_datetime(timestamp)

                # æå–OHLCVæ•°æ®
                if isinstance(row_data, dict) or len(row_data) >= 4:
                    open_price = float(row_data[0]) if len(row_data) > 0 else 0
                    high_price = float(row_data[1]) if len(row_data) > 1 else 0
                    low_price = float(row_data[2]) if len(row_data) > 2 else 0
                    close_price = float(row_data[3]) if len(row_data) > 3 else 0
                    volume = float(row_data[4]) if len(row_data) > 4 else 0

                    ohlc_data.append({
                        'date': date,
                        'open': open_price,
                        'high': high_price,
                        'low': low_price,
                        'close': close_price,
                        'volume': volume
                    })

            if not ohlc_data:
                logger.warning(f"XTQuant: æ— æœ‰æ•ˆåŸºå‡†OHLCVæ•°æ® {xt_symbol}")
                return pd.DataFrame()

            # åˆ›å»ºDataFrame
            result_df = pd.DataFrame(ohlc_data)
            result_df = result_df.sort_values('date').reset_index(drop=True)

            # è¿‡æ»¤æ—¥æœŸèŒƒå›´
            start_date_filtered = result_df['date'] >= start_dt
            end_date_filtered = result_df['date'] <= end_dt
            filtered_df = result_df[start_date_filtered & end_date_filtered].copy()

            if filtered_df.empty:
                logger.warning(f"XTQuant: è¿‡æ»¤åæ— åŸºå‡†æ•°æ® {xt_symbol}, åŸå§‹æ•°æ®èŒƒå›´: {result_df['date'].min()} åˆ° {result_df['date'].max()}")
                return pd.DataFrame()

            # éªŒè¯åŸºå‡†æ•°æ®è´¨é‡
            if not self._validate_xtquant_benchmark_data(filtered_df):
                logger.warning(f"XTQuant: åŸºå‡†æ•°æ®è´¨é‡éªŒè¯å¤±è´¥ {xt_symbol}")
                return pd.DataFrame()

            logger.info(f"XTQuant: æˆåŠŸè·å–åŸºå‡†æ•°æ® {symbol}, æ•°æ®é‡: {len(filtered_df)}")
            return filtered_df

        except Exception as e:
            logger.debug(f"XTQuantè·å–åŸºå‡†æ•°æ®å¤±è´¥ {symbol}: {type(e).__name__}: {e}")
            return pd.DataFrame()

    def _validate_xtquant_benchmark_data(self, data: pd.DataFrame) -> bool:
        """éªŒè¯XTQuantåŸºå‡†æ•°æ®è´¨é‡"""
        try:
            if data.empty:
                logger.error("XTQuantåŸºå‡†æ•°æ®éªŒè¯å¤±è´¥: DataFrameä¸ºç©º")
                return False

            # æ£€æŸ¥å¿…è¦åˆ—
            required_columns = ['date', 'close']
            for col in required_columns:
                if col not in data.columns:
                    logger.error(f"XTQuantåŸºå‡†æ•°æ®éªŒè¯å¤±è´¥: ç¼ºå°‘åˆ— {col}")
                    return False

            # æ£€æŸ¥ä»·æ ¼æ•°æ®æœ‰æ•ˆæ€§
            close_prices = data['close'].dropna()
            if len(close_prices) == 0:
                logger.error("XTQuantåŸºå‡†æ•°æ®éªŒè¯å¤±è´¥: æ— æœ‰æ•ˆä»·æ ¼æ•°æ®")
                return False

            # æ£€æŸ¥ä»·æ ¼æ˜¯å¦ä¸ºæ­£æ•°
            if (close_prices <= 0).any():
                logger.warning(f"XTQuantåŸºå‡†æ•°æ®è­¦å‘Š: å‘ç°éæ­£ä»·æ ¼ {(close_prices <= 0).sum()} ä¸ª")

            # æ£€æŸ¥æ•°æ®é‡
            if len(data) < 10:
                logger.warning(f"XTQuantåŸºå‡†æ•°æ®è­¦å‘Š: æ•°æ®ç‚¹è¾ƒå°‘ {len(data)} ä¸ªï¼Œå¯èƒ½å½±å“Betaè®¡ç®—")

            logger.debug(f"XTQuantåŸºå‡†æ•°æ®éªŒè¯é€šè¿‡: {len(data)} æ¡æ•°æ®, ä»·æ ¼èŒƒå›´ {close_prices.min():.4f} - {close_prices.max():.4f}")
            return True

        except Exception as e:
            logger.error(f"XTQuantåŸºå‡†æ•°æ®éªŒè¯è¿‡ç¨‹å¼‚å¸¸: {type(e).__name__}: {e}")
            return False

    def _try_wind(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """ä½¿ç”¨Wind APIè·å–æ•°æ®ï¼ˆé¢„ç•™æ¥å£ï¼‰"""
        # Windé›†æˆéœ€è¦ç‰¹æ®Šçš„æˆæƒå’Œå®‰è£…ï¼Œè¿™é‡Œæä¾›åŸºç¡€æ¡†æ¶
        logger.debug("Windæ¥å£æš‚æœªå®ç°")
        return pd.DataFrame()

    def _try_yfinance(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """ä½¿ç”¨yfinanceè·å–æ•°æ®ï¼ˆéœ€è¦pip install yfinanceï¼‰"""
        try:
            # å°è¯•å¯¼å…¥yfinance
            import yfinance as yf

            # è½¬æ¢æ—¥æœŸæ ¼å¼
            start_dt = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')

            # è½¬æ¢ç¬¦å·æ ¼å¼
            if symbol.startswith('51'):
                yf_symbol = f"{symbol}.SS"  # ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€
            elif symbol.startswith('15'):
                yf_symbol = f"{symbol}.SZ"  # æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€
            else:
                yf_symbol = f"{symbol}.SS"  # é»˜è®¤ä¸Šæµ·

            # ä¸‹è½½æ•°æ®
            data = yf.download(
                yf_symbol,
                start=start_dt.strftime('%Y-%m-%d'),
                end=end_dt.strftime('%Y-%m-%d'),
                progress=False,
                timeout=10  # è®¾ç½®è¶…æ—¶
            )

            if not data.empty:
                data = data.reset_index()
                # é‡å‘½ååˆ—ä»¥åŒ¹é…æ ‡å‡†æ ¼å¼
                data = data.rename(columns={
                    'Date': 'date',
                    'Open': 'open',
                    'High': 'high',
                    'Low': 'low',
                    'Close': 'close',
                    'Volume': 'volume'
                })
                return data
            else:
                return pd.DataFrame()

        except ImportError:
            logger.warning("yfinanceæœªå®‰è£…ï¼Œå¯ä»¥ä½¿ç”¨: pip install yfinance")
            return pd.DataFrame()
        except Exception as e:
            logger.debug(f"yfinanceè·å–å¤±è´¥: {e}")
            return pd.DataFrame()

    def _try_xtquant(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """ä½¿ç”¨XTQuantè·å–ETFæ•°æ®"""
        if not self.xtquant_initialized or not XTQUANT_AVAILABLE:
            logger.debug("XTQuantæœªåˆå§‹åŒ–æˆ–ä¸å¯ç”¨")
            return pd.DataFrame()

        try:
            logger.info(f"XTQuant: å¼€å§‹è·å–ETFæ•°æ® {symbol}")

            # è½¬æ¢æ—¥æœŸæ ¼å¼
            start_dt = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')

            # è½¬æ¢ETFä»£ç ä¸ºXTQuantæ ¼å¼
            xt_symbol = self._convert_to_xtquant_symbol(symbol)
            logger.debug(f"XTQuant: è½¬æ¢åä»£ç  {xt_symbol}")

            # é¦–å…ˆå°è¯•è·å–æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™å…ˆä¸‹è½½
            data = xtdata.get_market_data_ex([], [xt_symbol], period="1d", count=-1)

            if data is None or not data or xt_symbol not in data:
                logger.info(f"XTQuant: æœ¬åœ°æ— æ•°æ®ï¼Œå¼€å§‹ä¸‹è½½ {xt_symbol}")
                # ä¸‹è½½å†å²æ•°æ®
                xtdata.download_history_data(xt_symbol, period="1d", incrementally=True)
                # å†æ¬¡è·å–æ•°æ®
                data = xtdata.get_market_data_ex([], [xt_symbol], period="1d", count=-1)

            if not data or xt_symbol not in data:
                logger.warning(f"XTQuant: ä¸‹è½½åä»æ— æ³•è·å–æ•°æ® {xt_symbol}")
                return pd.DataFrame()

            # å¤„ç†XTQuantè¿”å›çš„æ•°æ®æ ¼å¼
            df = data[xt_symbol]

            # å°†æ•°æ®è½¬æ¢ä¸ºDataFrame
            ohlc_data = []
            for timestamp, row_data in df.items():
                # timestampæ˜¯æ—¶é—´æˆ³æ ¼å¼
                date = pd.to_datetime(timestamp)

                # æå–OHLCVæ•°æ®ï¼ˆæ ¹æ®XTQuantè¿”å›æ ¼å¼è°ƒæ•´ï¼‰
                if isinstance(row_data, dict) or len(row_data) >= 4:
                    open_price = float(row_data[0]) if len(row_data) > 0 else 0
                    high_price = float(row_data[1]) if len(row_data) > 1 else 0
                    low_price = float(row_data[2]) if len(row_data) > 2 else 0
                    close_price = float(row_data[3]) if len(row_data) > 3 else 0
                    volume = float(row_data[4]) if len(row_data) > 4 else 0

                    ohlc_data.append({
                        'date': date,
                        'open': open_price,
                        'high': high_price,
                        'low': low_price,
                        'close': close_price,
                        'volume': volume
                    })

            if not ohlc_data:
                logger.warning(f"XTQuant: æ— æœ‰æ•ˆOHLCVæ•°æ® {xt_symbol}")
                return pd.DataFrame()

            # åˆ›å»ºDataFrame
            result_df = pd.DataFrame(ohlc_data)
            result_df = result_df.sort_values('date').reset_index(drop=True)

            # è¿‡æ»¤æ—¥æœŸèŒƒå›´
            start_date_filtered = result_df['date'] >= start_dt
            end_date_filtered = result_df['date'] <= end_dt
            filtered_df = result_df[start_date_filtered & end_date_filtered].copy()

            if filtered_df.empty:
                logger.warning(f"XTQuant: è¿‡æ»¤åæ— æ•°æ® {xt_symbol}, åŸå§‹æ•°æ®èŒƒå›´: {result_df['date'].min()} åˆ° {result_df['date'].max()}")
                return pd.DataFrame()

            logger.info(f"XTQuant: æˆåŠŸè·å–ETFæ•°æ® {symbol}, æ•°æ®é‡: {len(filtered_df)}")
            return filtered_df

        except Exception as e:
            logger.error(f"XTQuantè·å–ETFæ•°æ®å¤±è´¥ {symbol}: {type(e).__name__}: {e}")
            return pd.DataFrame()

    def _convert_to_xtquant_symbol(self, symbol: str) -> str:
        """è½¬æ¢ETF/è‚¡ç¥¨ä»£ç ä¸ºXTQuantæ ¼å¼"""
        # ç§»é™¤å¯èƒ½çš„å‰ç¼€å’Œåç¼€
        clean_symbol = symbol.replace('etf', '').replace('ETF', '').replace('SH', '').replace('SZ', '')

        # æ ¹æ®ä»£ç å‰ç¼€ç¡®å®šå¸‚åœºå’Œæ ¼å¼
        if symbol.startswith(('51', '58', '56')):  # ä¸Šæµ·å¸‚åœºETF
            return f"{clean_symbol}.SZ"  # XTQuantä¸­ä¸Šæµ·ETFä½¿ç”¨.SZåç¼€
        elif symbol.startswith(('15', '16', '159')):  # æ·±åœ³å¸‚åœºETF
            return f"{clean_symbol}.SZ"
        elif symbol.startswith(('600', '601', '603', '605')):  # ä¸Šæµ·Aè‚¡
            return f"{clean_symbol}.SH"
        elif symbol.startswith(('000', '001', '002', '003')):  # æ·±åœ³Aè‚¡
            return f"{clean_symbol}.SZ"
        elif symbol == "000300":  # æ²ªæ·±300æŒ‡æ•°
            return "000300.SH"
        elif symbol.startswith('000'):  # æ·±åœ³æŒ‡æ•°
            return f"{symbol}.SZ"
        else:
            # é»˜è®¤å°è¯•æ·±åœ³å¸‚åœº
            return f"{clean_symbol}.SZ"

    def _clean_and_standardize_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—å’Œæ ‡å‡†åŒ–æ•°æ®"""
        # é‡å‘½ååˆ—
        column_mapping = {
            'æ—¥æœŸ': 'date',
            'å¼€ç›˜': 'open',
            'æ”¶ç›˜': 'close',
            'æœ€é«˜': 'high',
            'æœ€ä½': 'low',
            'æˆäº¤é‡': 'volume',
            'æˆäº¤é¢': 'amount',
            'Date': 'date',
            'Open': 'open',
            'Close': 'close',
            'High': 'high',
            'Low': 'low',
            'Volume': 'volume',
            'Amount': 'amount'
        }

        # æŸ¥æ‰¾å®é™…å­˜åœ¨çš„åˆ—
        existing_columns = {}
        for chinese_col, english_col in column_mapping.items():
            for col in data.columns:
                if chinese_col in col or english_col in col:
                    existing_columns[col] = english_col
                    break

        data = data.rename(columns=existing_columns)

        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        required_columns = ['date', 'open', 'high', 'low', 'close']
        for col in required_columns:
            if col not in data.columns:
                logger.error(f"ç¼ºå°‘å¿…è¦åˆ—: {col}")
                raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {col}")

        # æ•°æ®ç±»å‹è½¬æ¢
        data['date'] = pd.to_datetime(data['date'])
        numeric_columns = ['open', 'high', 'low', 'close']
        for col in numeric_columns:
            data[col] = pd.to_numeric(data[col], errors='coerce')

        # å¤„ç†æˆäº¤é‡
        if 'volume' in data.columns:
            data['volume'] = pd.to_numeric(data['volume'], errors='coerce')
        else:
            data['volume'] = 0

        # å¤„ç†æˆäº¤é¢
        if 'amount' in data.columns:
            data['amount'] = pd.to_numeric(data['amount'], errors='coerce')
        else:
            data['amount'] = 0

        # æŒ‰æ—¥æœŸæ’åº
        data = data.sort_values('date').reset_index(drop=True)

        # ç§»é™¤é‡å¤å’Œç©ºå€¼
        data = data.drop_duplicates(subset=['date'])
        data = data.dropna(subset=required_columns)

        # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
        data = self._add_technical_indicators(data)

        return data

    def _add_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ æŠ€æœ¯æŒ‡æ ‡"""
        if len(data) < 2:
            return data

        # è®¡ç®—æ”¶ç›Šç‡
        data['returns'] = data['close'].pct_change()

        # è®¡ç®—ç§»åŠ¨å¹³å‡çº¿
        if len(data) >= 5:
            data['ma5'] = data['close'].rolling(window=5).mean()
        else:
            data['ma5'] = np.nan

        if len(data) >= 10:
            data['ma10'] = data['close'].rolling(window=10).mean()
        else:
            data['ma10'] = np.nan

        if len(data) >= 20:
            data['ma20'] = data['close'].rolling(window=20).mean()
        else:
            data['ma20'] = np.nan

        if len(data) >= 60:
            data['ma60'] = data['close'].rolling(window=60).mean()
        else:
            data['ma60'] = np.nan

        # è®¡ç®—æ³¢åŠ¨ç‡
        if len(data) >= 20:
            data['volatility_20'] = data['returns'].rolling(window=20).std() * np.sqrt(252)
        else:
            data['volatility_20'] = np.nan

        # è®¡ç®—RSI
        data['rsi_14'] = self._calculate_rsi(data['close'], 14)

        return data

    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """è®¡ç®—RSIæŒ‡æ ‡"""
        if len(prices) < period + 1:
            return pd.Series([np.nan] * len(prices))

        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi

    def _get_cache_path(self, cache_key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{cache_key}.pkl"

    def _load_from_cache(self, cache_key: str) -> Optional[pd.DataFrame]:
        """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
        cache_path = self._get_cache_path(cache_key)

        if not cache_path.exists():
            return None

        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        cache_time = cache_path.stat().st_mtime
        current_time = datetime.now().timestamp()

        if current_time - cache_time > self.cache_ttl:
            return None

        try:
            with open(cache_path, 'rb') as f:
                data = pickle.load(f)
                logger.debug(f"ä»ç¼“å­˜åŠ è½½: {cache_key}, æ•°æ®é‡: {len(data)}")
                return data
        except Exception as e:
            logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
            return None

    def _save_to_cache(self, cache_key: str, data: pd.DataFrame):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        cache_path = self._get_cache_path(cache_key)

        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
            logger.debug(f"ä¿å­˜åˆ°ç¼“å­˜: {cache_key}, æ•°æ®é‡: {len(data)}")
        except Exception as e:
            logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def batch_fetch_etf_data(self, symbols: List[str], start_date: str = None, end_date: str = None) -> Dict[str, pd.DataFrame]:
        """æ‰¹é‡è·å–ETFæ•°æ®"""
        results = {}

        for symbol in symbols:
            try:
                data = self.fetch_etf_data(symbol, start_date, end_date)
                if not data.empty:
                    results[symbol] = data
                    logger.info(f"æˆåŠŸè·å– {symbol}: {len(data)} æ¡æ•°æ®")
                else:
                    logger.warning(f"æœªè·å–åˆ°æ•°æ®: {symbol}")
            except Exception as e:
                logger.error(f"è·å–æ•°æ®å¤±è´¥ {symbol}: {e}")

        logger.info(f"æ‰¹é‡è·å–å®Œæˆ: {len(results)}/{len(symbols)} ä¸ªETFæˆåŠŸ")
        return results

    def get_data_quality_report(self, data: pd.DataFrame) -> Dict:
        """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
        if data.empty:
            return {"error": "æ•°æ®ä¸ºç©º"}

        report = {
            "total_records": len(data),
            "date_range": {
                "start": data['date'].min().strftime('%Y-%m-%d'),
                "end": data['date'].max().strftime('%Y-%m-%d')
            },
            "missing_values": data.isnull().sum().to_dict(),
            "price_stats": {
                "min_price": float(data['close'].min()),
                "max_price": float(data['close'].max()),
                "avg_price": float(data['close'].mean()),
                "price_volatility": float(data['close'].std() / data['close'].mean() if data['close'].mean() != 0 else 0)
            },
            "trading_days": len(data),
            "data_completeness": float((1 - data.isnull().sum().sum() / (len(data) * len(data.columns))) * 100) if len(data) > 0 else 0
        }

        return report

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        try:
            cache_files = list(self.cache_dir.glob("*.pkl"))
            for cache_file in cache_files:
                cache_file.unlink()
            logger.info(f"ç¼“å­˜å·²æ¸…ç©ºï¼Œåˆ é™¤ {len(cache_files)} ä¸ªæ–‡ä»¶")
        except Exception as e:
            logger.error(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")

    def get_cache_info(self) -> Dict:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        cache_files = list(self.cache_dir.glob("*.pkl"))
        info = {
            "cache_dir": str(self.cache_dir),
            "total_files": len(cache_files),
            "files": [f.name for f in cache_files[:10]]  # æ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶
        }
        return info

    def _validate_benchmark_data_quality(self, data: pd.DataFrame) -> bool:
        """éªŒè¯åŸºå‡†æ•°æ®è´¨é‡"""
        try:
            if data.empty:
                logger.error("ğŸ“Š æ•°æ®éªŒè¯å¤±è´¥: DataFrameä¸ºç©º")
                return False

            required_columns = ['date', 'close']
            missing_columns = [col for col in required_columns if col not in data.columns]
            if missing_columns:
                logger.error(f"ğŸ“Š æ•°æ®éªŒè¯å¤±è´¥: ç¼ºå°‘å¿…è¦åˆ— {missing_columns}")
                logger.error(f"ğŸ“Š ç°æœ‰åˆ—: {list(data.columns)}")
                return False

            # æ•°æ®è´¨é‡æ£€æŸ¥
            data_shape = data.shape
            date_range = f"{data['date'].min()} åˆ° {data['date'].max()}" if pd.api.types.is_datetime64_any_dtype(data['date']) else "æ—¥æœŸæ ¼å¼å¼‚å¸¸"

            # æ£€æŸ¥æ•°æ®ç‚¹æ•°é‡
            if data_shape[0] < 10:
                logger.warning(f"ğŸ“Š æ•°æ®ç‚¹è¾ƒå°‘ ({data_shape[0]} æ¡)ï¼Œå¯èƒ½å½±å“Betaè®¡ç®—å‡†ç¡®æ€§")

            # æ£€æŸ¥ä»·æ ¼æ•°æ®æœ‰æ•ˆæ€§
            if 'close' in data.columns:
                close_data = data['close'].dropna()
                if len(close_data) == 0:
                    logger.error("ğŸ“Š æ•°æ®éªŒè¯å¤±è´¥: æ”¶ç›˜ä»·å…¨éƒ¨ä¸ºç©ºå€¼")
                    return False

                if (close_data <= 0).any():
                    logger.warning(f"ğŸ“Š å‘ç°éæ­£æ”¶ç›˜ä»·æ•°æ®ï¼Œå…±æœ‰ {(close_data <= 0).sum()} ä¸ªå¼‚å¸¸å€¼")

                # æ£€æŸ¥ä»·æ ¼ç¨³å®šæ€§
                price_std = close_data.std()
                price_mean = close_data.mean()
                if price_std == 0:
                    logger.warning("ğŸ“Š ä»·æ ¼æ•°æ®æ— æ³¢åŠ¨ï¼Œæ‰€æœ‰å€¼ç›¸åŒ")

            # è¯¦ç»†è´¨é‡æŠ¥å‘Š
            logger.info(f"ğŸ“Š æ•°æ®è´¨é‡éªŒè¯é€šè¿‡:")
            logger.info(f"  ğŸ“ˆ æ•°æ®å½¢çŠ¶: {data_shape}")
            logger.info(f"  ğŸ“… æ—¶é—´èŒƒå›´: {date_range}")
            logger.info(f"  ğŸ“‹ æ•°æ®åˆ—: {list(data.columns)}")
            logger.info(f"  âŒ ç¼ºå¤±å€¼ç»Ÿè®¡: {data.isnull().sum().to_dict()}")

            # æ£€æŸ¥æ—¥æœŸè¿ç»­æ€§
            if pd.api.types.is_datetime64_any_dtype(data['date']):
                data_sorted = data.sort_values('date')
                expected_days = (data_sorted['date'].iloc[-1] - data_sorted['date'].iloc[0]).days + 1
                actual_days = len(data_sorted)
                if actual_days < expected_days * 0.7:  # å…è®¸30%çš„ç¼ºå¤±ï¼ˆå‘¨æœ«ã€èŠ‚å‡æ—¥ï¼‰
                    logger.warning(f"ğŸ“Š æ—¥æœŸè¿ç»­æ€§: å®é™… {actual_days} å¤©ï¼Œé¢„æœŸçº¦ {expected_days} å¤©")

            return True

        except Exception as e:
            logger.error(f"ğŸ“Š æ•°æ®éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {type(e).__name__}: {e}")
            return False

    # ==================== XTQuant å®æ—¶è®¢é˜…åŠŸèƒ½ ====================

    def subscribe_xtquant_realtime(self, symbols: List[str], callback=None, period: str = "1d"):
        """
        ä½¿ç”¨XTQuantè®¢é˜…å®æ—¶æ•°æ®

        Args:
            symbols: è¯åˆ¸ä»£ç åˆ—è¡¨
            callback: å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶(data)å‚æ•°
            period: æ•°æ®å‘¨æœŸï¼Œé»˜è®¤"1d"

        Returns:
            è®¢é˜…æˆåŠŸçŠ¶æ€
        """
        if not self.xtquant_initialized or not XTQUANT_AVAILABLE:
            logger.error("XTQuantæœªåˆå§‹åŒ–ï¼Œæ— æ³•è®¢é˜…å®æ—¶æ•°æ®")
            return False

        try:
            # è½¬æ¢ç¬¦å·æ ¼å¼
            xt_symbols = [self._convert_to_xtquant_symbol(symbol) for symbol in symbols]

            logger.info(f"XTQuant: å¼€å§‹è®¢é˜…å®æ—¶æ•°æ® {xt_symbols}")

            # å­˜å‚¨è®¢é˜…çŠ¶æ€
            if not hasattr(self, '_xtquant_subscriptions'):
                self._xtquant_subscriptions = {}

            for symbol, xt_symbol in zip(symbols, xt_symbols):
                try:
                    # å…ˆä¸‹è½½å†å²æ•°æ®ç¡®ä¿æœ‰åŸºç¡€æ•°æ®
                    xtdata.download_history_data(xt_symbol, period=period, incrementally=True)

                    # è®¢é˜…å®æ—¶æ•°æ®
                    if callback:
                        xtdata.subscribe_quote(xt_symbol, period=period, count=-1, callback=callback)
                    else:
                        # ä½¿ç”¨é»˜è®¤å›è°ƒå‡½æ•°
                        default_callback = self._default_xtquant_callback
                        xtdata.subscribe_quote(xt_symbol, period=period, count=-1, callback=default_callback)

                    self._xtquant_subscriptions[symbol] = {
                        'xt_symbol': xt_symbol,
                        'period': period,
                        'callback': callback or default_callback,
                        'subscribed_at': datetime.now()
                    }

                    logger.info(f"XTQuant: æˆåŠŸè®¢é˜… {symbol} ({xt_symbol})")

                except Exception as e:
                    logger.error(f"XTQuant: è®¢é˜…å¤±è´¥ {symbol} ({xt_symbol}): {e}")
                    continue

            logger.info(f"XTQuant: å®æ—¶è®¢é˜…å®Œæˆï¼ŒæˆåŠŸè®¢é˜… {len(self._xtquant_subscriptions)}/{len(symbols)} ä¸ªæ ‡çš„")
            return len(self._xtquant_subscriptions) > 0

        except Exception as e:
            logger.error(f"XTQuant: å®æ—¶è®¢é˜…å¼‚å¸¸: {type(e).__name__}: {e}")
            return False

    def _default_xtquant_callback(self, data):
        """
        XTQuanté»˜è®¤å›è°ƒå‡½æ•°

        Args:
            data: XTQuantæ¨é€çš„æ•°æ®
        """
        try:
            if not data:
                return

            # è§£æå›è°ƒæ•°æ®
            symbol_list = list(data.keys())
            if not symbol_list:
                return

            for xt_symbol in symbol_list:
                symbol_data = data[xt_symbol]
                if symbol_data is None:
                    continue

                # å°è¯•è·å–æœ€æ–°ä»·æ ¼æ•°æ®
                try:
                    latest_data = xtdata.get_market_data_ex([], [xt_symbol], period="1d", count=1)
                    if latest_data and xt_symbol in latest_data:
                        latest_df = latest_data[xt_symbol]
                        if not latest_df.empty:
                            # è·å–æœ€æ–°ä¸€æ¡æ•°æ®
                            latest_timestamp = list(latest_df.keys())[-1]
                            latest_price_data = latest_df[latest_timestamp]

                            # è½¬æ¢ä¸ºå¯è¯»æ ¼å¼
                            current_price = float(latest_price_data[3]) if len(latest_price_data) > 3 else 0
                            current_time = pd.to_datetime(latest_timestamp)

                            logger.info(f"XTQuantå®æ—¶æ•°æ®: {xt_symbol} ä»·æ ¼:{current_price:.4f} æ—¶é—´:{current_time}")

                except Exception as parse_e:
                    logger.debug(f"XTQuantå›è°ƒæ•°æ®è§£æå¤±è´¥: {parse_e}")

        except Exception as e:
            logger.error(f"XTQuanté»˜è®¤å›è°ƒå¼‚å¸¸: {type(e).__name__}: {e}")

    def unsubscribe_xtquant_realtime(self, symbols: List[str] = None):
        """
        å–æ¶ˆXTQuantå®æ—¶æ•°æ®è®¢é˜…

        Args:
            symbols: è¦å–æ¶ˆè®¢é˜…çš„è¯åˆ¸ä»£ç åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå–æ¶ˆæ‰€æœ‰
        """
        if not hasattr(self, '_xtquant_subscriptions') or not self._xtquant_subscriptions:
            logger.info("XTQuant: æ— æ´»è·ƒè®¢é˜…")
            return

        try:
            if symbols is None:
                # å–æ¶ˆæ‰€æœ‰è®¢é˜…
                symbols_to_unsubscribe = list(self._xtquant_subscriptions.keys())
            else:
                # åªå–æ¶ˆæŒ‡å®šçš„è®¢é˜…
                symbols_to_unsubscribe = [s for s in symbols if s in self._xtquant_subscriptions]

            unsubscribed_count = 0
            for symbol in symbols_to_unsubscribe:
                if symbol in self._xtquant_subscriptions:
                    xt_symbol = self._xtquant_subscriptions[symbol]['xt_symbol']
                    try:
                        # XTQuantå–æ¶ˆè®¢é˜…
                        if hasattr(xtdata, 'unsubscribe_quote'):
                            xtdata.unsubscribe_quote(xt_symbol)

                        del self._xtquant_subscriptions[symbol]
                        unsubscribed_count += 1
                        logger.info(f"XTQuant: å–æ¶ˆè®¢é˜…æˆåŠŸ {symbol} ({xt_symbol})")

                    except Exception as e:
                        logger.error(f"XTQuant: å–æ¶ˆè®¢é˜…å¤±è´¥ {symbol}: {e}")

            logger.info(f"XTQuant: å–æ¶ˆè®¢é˜…å®Œæˆï¼ŒæˆåŠŸå–æ¶ˆ {unsubscribed_count}/{len(symbols_to_unsubscribe)} ä¸ªè®¢é˜…")

        except Exception as e:
            logger.error(f"XTQuant: å–æ¶ˆè®¢é˜…å¼‚å¸¸: {type(e).__name__}: {e}")

    def get_xtquant_subscription_status(self) -> Dict:
        """
        è·å–XTQuantå®æ—¶è®¢é˜…çŠ¶æ€

        Returns:
            è®¢é˜…çŠ¶æ€ä¿¡æ¯
        """
        if not hasattr(self, '_xtquant_subscriptions'):
            return {
                'enabled': False,
                'total_subscriptions': 0,
                'subscriptions': []
            }

        status = {
            'enabled': len(self._xtquant_subscriptions) > 0,
            'total_subscriptions': len(self._xtquant_subscriptions),
            'xtquant_initialized': self.xtquant_initialized,
            'subscriptions': []
        }

        for symbol, info in self._xtquant_subscriptions.items():
            subscription_time = info['subscribed_at']
            duration = datetime.now() - subscription_time

            status['subscriptions'].append({
                'symbol': symbol,
                'xt_symbol': info['xt_symbol'],
                'period': info['period'],
                'subscribed_at': subscription_time.strftime('%Y-%m-%d %H:%M:%S'),
                'duration_seconds': int(duration.total_seconds())
            })

        return status

    def start_xtquant_realtime_loop(self):
        """
        å¯åŠ¨XTQuantå®æ—¶æ•°æ®å¾ªç¯ï¼ˆé˜»å¡æ¨¡å¼ï¼‰

        æ³¨æ„ï¼šæ­¤æ–¹æ³•ä¼šé˜»å¡å½“å‰çº¿ç¨‹ï¼Œé€‚åˆåœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œ
        """
        if not self.xtquant_initialized or not XTQUANT_AVAILABLE:
            logger.error("XTQuantæœªåˆå§‹åŒ–ï¼Œæ— æ³•å¯åŠ¨å®æ—¶å¾ªç¯")
            return

        if not hasattr(self, '_xtquant_subscriptions') or not self._xtquant_subscriptions:
            logger.warning("XTQuant: æ— æ´»è·ƒè®¢é˜…ï¼Œä½†å°†å¯åŠ¨å®æ—¶å¾ªç¯ç­‰å¾…è®¢é˜…")
        else:
            logger.info(f"XTQuant: å¯åŠ¨å®æ—¶å¾ªç¯ï¼Œç›‘æ§ {len(self._xtquant_subscriptions)} ä¸ªè®¢é˜…")

        try:
            logger.info("XTQuant: å®æ—¶æ•°æ®å¾ªç¯å·²å¯åŠ¨ï¼ŒæŒ‰Ctrl+Cåœæ­¢...")
            # å¯åŠ¨XTQuantå®æ—¶æ•°æ®å¾ªç¯
            xtdata.run()

        except KeyboardInterrupt:
            logger.info("XTQuant: ç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢å®æ—¶æ•°æ®å¾ªç¯")
        except Exception as e:
            logger.error(f"XTQuant: å®æ—¶å¾ªç¯å¼‚å¸¸: {type(e).__name__}: {e}")
        finally:
            logger.info("XTQuant: å®æ—¶æ•°æ®å¾ªç¯å·²åœæ­¢")


def main():
    """æµ‹è¯•å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    fetcher = MarketDataFetcher()

    # æµ‹è¯•è·å–å•ä¸ªETFæ•°æ®
    test_symbols = ["159682", "510300", "512880"]  # ç§‘åˆ›50ETF, æ²ªæ·±300ETF, è¯åˆ¸ETF

    for symbol in test_symbols:
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•è·å–ETFæ•°æ®: {symbol}")
        print('='*60)

        data = fetcher.fetch_etf_data(symbol, start_date="20230101", end_date="20240101")

        if not data.empty:
            print(f"æˆåŠŸè·å– {len(data)} æ¡æ•°æ®")
            print("æ•°æ®é¢„è§ˆ:")
            print(data[['date', 'open', 'high', 'low', 'close', 'volume']].head())
            print("...")
            print(data[['date', 'open', 'high', 'low', 'close', 'volume']].tail())

            # ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
            report = fetcher.get_data_quality_report(data)
            print("\næ•°æ®è´¨é‡æŠ¥å‘Š:")
            for key, value in report.items():
                if isinstance(value, dict):
                    print(f"  {key}:")
                    for k, v in value.items():
                        print(f"    {k}: {v}")
                else:
                    print(f"  {key}: {value}")
        else:
            print("è·å–æ•°æ®å¤±è´¥")

    # æ‰¹é‡è·å–æµ‹è¯•
    print(f"\n{'='*60}")
    print("æ‰¹é‡è·å–æµ‹è¯•")
    print('='*60)

    batch_data = fetcher.batch_fetch_etf_data(test_symbols, start_date="20231001", end_date="20240101")
    print(f"æ‰¹é‡è·å–ç»“æœ: {len(batch_data)}/{len(test_symbols)} æˆåŠŸ")

    # ç¼“å­˜ä¿¡æ¯
    print(f"\n{'='*60}")
    print("ç¼“å­˜ä¿¡æ¯")
    print('='*60)
    cache_info = fetcher.get_cache_info()
    for key, value in cache_info.items():
        if isinstance(value, list):
            print(f"{key}: {len(value)} ä¸ªæ–‡ä»¶")
            for file in value:
                print(f"  - {file}")
        else:
            print(f"{key}: {value}")

    # æ¸…ç©ºç¼“å­˜æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
    # fetcher.clear_cache()


if __name__ == "__main__":
    main()